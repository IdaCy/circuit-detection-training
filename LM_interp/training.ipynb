{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Neural Circuits in GPT-2 using Clustering and Dimensionality Reduction\n",
    "In this notebook, we analyze the inner workings, or \"circuits,\" of GPT-2, a language model from the Transformers family, by examining its hidden states and attention patterns. We'll use techniques like **Principal Component Analysis (PCA)** and **K-Means clustering** to reduce the dimensionality of the hidden states and identify clusters, which may correspond to functional circuits in the model.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Imports and Model Setup\n",
    "We start by importing the necessary libraries and loading the GPT-2 model and tokenizer. The model will run in evaluation mode to prevent any training operations and ensure consistent results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Model(\n",
       "  (wte): Embedding(50257, 768)\n",
       "  (wpe): Embedding(1024, 768)\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (h): ModuleList(\n",
       "    (0-11): 12 x GPT2Block(\n",
       "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (attn): GPT2SdpaAttention(\n",
       "        (c_attn): Conv1D(nf=2304, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=768)\n",
       "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (mlp): GPT2MLP(\n",
       "        (c_fc): Conv1D(nf=3072, nx=768)\n",
       "        (c_proj): Conv1D(nf=768, nx=3072)\n",
       "        (act): NewGELUActivation()\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries for model, clustering, and visualization\n",
    "import torch\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "\n",
    "# Load a pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2Model.from_pretrained(model_name, output_attentions=True, output_hidden_states=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Function to Capture Model Outputs\n",
    "We'll define a helper function, `get_model_outputs`, that processes an input sentence and extracts **attention weights** and **hidden states** from each layer of the model.\n",
    "\n",
    "- **Attention weights** show which words (tokens) are considered important at each step.\n",
    "- **Hidden states** represent the internal embeddings at each layer, capturing semantic and syntactic information about the sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to capture activations and attentions for a given sentence\n",
    "def get_model_outputs(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    attentions = outputs.attentions  # List of attention tensors for each layer\n",
    "    hidden_states = outputs.hidden_states  # List of hidden state tensors for each layer\n",
    "    return attentions, hidden_states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dimensionality Reduction and Clustering\n",
    "To detect neural circuits, we'll reduce the dimensionality of the hidden states for a selected layer using **Principal Component Analysis (PCA)** and then cluster the data with **K-Means**. This will allow us to identify groups of neurons (or activations) that may correspond to specific roles in processing language.\n",
    "\n",
    "### Function: `find_circuits`\n",
    "- **PCA** reduces the dimensionality of hidden states, helping visualize the circuits and simplifying the clustering.\n",
    "- **K-Means clustering** groups the neurons into clusters, which may correspond to circuits or functional groups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA and K-means to find circuits\n",
    "def find_circuits(hidden_states, layer_index, n_components=10, n_clusters=5):\n",
    "    \"\"\"\n",
    "    Apply PCA and K-means clustering on hidden states for a given layer.\n",
    "    \n",
    "    Parameters:\n",
    "        hidden_states (list): List of hidden state tensors for each layer.\n",
    "        layer_index (int): Index of the layer to analyze.\n",
    "        n_components (int): Desired number of PCA components.\n",
    "        n_clusters (int): Number of clusters to find with K-means.\n",
    "        \n",
    "    Returns:\n",
    "        cluster_labels (np.array): Array with cluster assignments for each neuron.\n",
    "    \"\"\"\n",
    "    layer_activations = hidden_states[layer_index].squeeze(0).detach().cpu().numpy()  # Shape: (seq_len, hidden_dim)\n",
    "\n",
    "    # Flatten across the sequence length\n",
    "    flattened_activations = layer_activations.reshape(-1, layer_activations.shape[-1])\n",
    "\n",
    "    # Set n_components dynamically based on data size\n",
    "    n_components = min(n_components, flattened_activations.shape[0], flattened_activations.shape[1])\n",
    "\n",
    "    # Dimensionality reduction using PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_activations = pca.fit_transform(flattened_activations)\n",
    "\n",
    "    # Clustering with KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    cluster_labels = kmeans.fit_predict(reduced_activations)\n",
    "\n",
    "    # Print cluster information\n",
    "    print(f\"Layer {layer_index}: Circuit Clusters Distribution - {np.bincount(cluster_labels)}\")\n",
    "    return cluster_labels, reduced_activations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualizing the Clusters with t-SNE\n",
    "The hidden states are clustered into groups, and **t-SNE** is used to visualize these clusters in a 2D space. This visualization can reveal if certain words or parts of sentences activate specific neural circuits within the model.\n",
    "\n",
    "### Function: `visualize_clusters`\n",
    "This function applies t-SNE to reduce the hidden states further for visualization and then plots the clusters in 2D.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(reduced_activations, cluster_labels, sentence, layer_name):\n",
    "    # Dynamically set perplexity to be less than the number of samples\n",
    "    n_samples = reduced_activations.shape[0]\n",
    "    perplexity = min(30, n_samples - 1)  # Ensure perplexity is less than n_samples\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=0, perplexity=perplexity)\n",
    "    tsne_result = tsne.fit_transform(reduced_activations)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    scatter = plt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=cluster_labels, cmap='viridis')\n",
    "    plt.colorbar(scatter, label=\"Cluster\")\n",
    "    plt.title(f\"Cluster Visualization for {layer_name}\\nSentence: {sentence}\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "    # Create a filename based on the layer and a shortened version of the sentence\n",
    "    short_sentence = \"_\".join(sentence.split()[:5])  # Use the first 5 words as part of the filename\n",
    "    filename = f\"{layer_name}_{short_sentence}.png\"\n",
    "    \n",
    "    # Save the plot\n",
    "    output_dir = \"output\"\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create directory if it doesn't exist\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()  # Close the figure to free up memory\n",
    "    print(f\"Saved cluster visualization to {filepath}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyzing Multiple Sentences\n",
    "To observe how circuits activate for different inputs, we'll analyze several sentences. Weâ€™ll apply the functions defined above to:\n",
    "1. Extract hidden states and attentions.\n",
    "2. Perform PCA and clustering on selected layers.\n",
    "3. Visualize clusters in 2D using t-SNE.\n",
    "\n",
    "This allows us to compare the circuits activated by different sentence structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`GPT2SdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing sentence: 'The quick brown fox jumps over the lazy dog.'\n",
      "Layer 10: Circuit Clusters Distribution - [1 1 2 2 4]\n",
      "Cluster labels for layer 10: [1 2 2 4 4 3 3 0 4 4]\n",
      "\n",
      "Saved cluster visualization to cluster_visualizations/Layer 10_The_quick_brown_fox_jumps.png\n",
      "Analyzing sentence: 'She sells sea shells by the sea shore.'\n",
      "Layer 10: Circuit Clusters Distribution - [3 1 2 2 1]\n",
      "Cluster labels for layer 10: [1 0 4 0 3 3 2 2 0]\n",
      "\n",
      "Saved cluster visualization to cluster_visualizations/Layer 10_She_sells_sea_shells_by.png\n",
      "Analyzing sentence: 'Artificial intelligence is transforming the world.'\n",
      "Layer 10: Circuit Clusters Distribution - [1 2 2 1 2]\n",
      "Cluster labels for layer 10: [0 2 2 4 1 1 3 4]\n",
      "\n",
      "Saved cluster visualization to cluster_visualizations/Layer 10_Artificial_intelligence_is_transforming_the.png\n"
     ]
    }
   ],
   "source": [
    "# Test multiple sentences to compare circuit activation\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"She sells sea shells by the sea shore.\",\n",
    "    \"Artificial intelligence is transforming the world.\"\n",
    "]\n",
    "\n",
    "# Analyze each sentence\n",
    "for sentence in sentences:\n",
    "    print(f\"Analyzing sentence: '{sentence}'\")\n",
    "    # Get model outputs for the current sentence\n",
    "    attentions, hidden_states = get_model_outputs(sentence)\n",
    "\n",
    "    # Analyze circuits for a specific layer\n",
    "    layer_index = 10  # Choose a layer, e.g., layer 10\n",
    "    cluster_labels, reduced_activations = find_circuits(hidden_states, layer_index)\n",
    "    print(f\"Cluster labels for layer {layer_index}: {cluster_labels}\\n\")\n",
    "\n",
    "    # Visualize clusters\n",
    "    visualize_clusters(reduced_activations, cluster_labels, sentence, f\"Layer {layer_index}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Ideas\n",
    "\n",
    "Now that you've visualized neural circuits for different sentences, here are some ideas to deepen your analysis:\n",
    "\n",
    "**Compare Layers**: Try analyzing different layers (e.g., `layer_index = 5` or `layer_index = 11`) to see how circuits differ across layers. Early layers might focus on simpler linguistic structures, while deeper layers capture more complex relationships.\n",
    "\n",
    "**Adjust Clustering Parameters**: Experiment with the number of clusters (`n_clusters`) and PCA components (`n_components`) to see how the model groups neurons differently. Does changing `n_clusters` result in more meaningful or interpretable circuits?\n",
    "\n",
    "**Analyze Sentence Structure Effects**: Test sentences with different structures or topics. For example:\n",
    "   - Short versus long sentences.\n",
    "   - Questions versus statements.\n",
    "   - Sentences with technical vocabulary versus common language.\n",
    "\n",
    "**Visualize Attention Patterns**: Although this notebook focuses on hidden states, attention patterns are another valuable part of interpretability. Explore the `attentions` output to understand how the model distributes attention across tokens.\n",
    "\n",
    "**Use Other Models**: Swap out GPT-2 with other transformer models (e.g., GPT-3, BERT, or T5) to see if their circuits activate differently for the same sentences. You can replace `GPT2Model` with other transformer models available in the `transformers` library.\n",
    "\n",
    "**Experiment with Dimensionality Reduction**: Instead of PCA, try other dimensionality reduction techniques like `UMAP` or adjust the perplexity in t-SNE to see how the visualization changes.\n",
    "\n",
    "Enjoy exploring!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "circenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
